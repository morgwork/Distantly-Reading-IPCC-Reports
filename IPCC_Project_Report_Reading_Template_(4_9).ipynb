{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IPCC Project_Report Reading Template (4_9)",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPCaaSAm0MmX6/LMzxrAD5a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morgwork/Distantly-Reading-IPCC-Reports/blob/main/IPCC_Project_Report_Reading_Template_(4_9).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this codebook, I will convert select PDF files derived from the *IPCC* archive linked [here](https://archive.ipcc.ch/). The archive covers the first five assessment cycles. The [sixth](https://www.ipcc.ch/assessment-report/ar6/) is being drafted, and I may not include it in the corpus (more on this in the project write-up). I plan to use this codebook as a template for reading reports, meaning I will copy it as multiple files to handle the multiple reports.**"
      ],
      "metadata": {
        "id": "NPum-L1dopPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Conversion\n",
        "In this section I convert the PDF report to a machine-readble string. Most of this code is from an external library, so I put links in as reference. The code's ultimate output is a string for subsequent cleaning and analysis."
      ],
      "metadata": {
        "id": "Pt8lmTZSMGzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# opens PDF. I suppose this is just to make sure it can be read...realize now that it's separate libraries\n",
        "\n",
        "#install pyDF2\n",
        "! pip install PyPDF2\n",
        "\n",
        "# importing all the required modules\n",
        "import PyPDF2\n",
        "\n",
        "# creating an object \n",
        "file = open('', 'rb') # put file here \n",
        "\n",
        "# creating a pdf reader object\n",
        "fileReader = PyPDF2.PdfFileReader(file)\n",
        "\n",
        "# print the number of pages in pdf file\n",
        "print(fileReader.numPages) # fileReader becomes object"
      ],
      "metadata": {
        "id": "wlUlkAPHuxwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pdfminer # successful installation of PDFminer\n",
        "import pdfminer"
      ],
      "metadata": {
        "id": "c5btcmjcsyRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Got the following code from: https://stackoverflow.com/questions/5725278/how-do-i-use-pdfminer-as-a-library/8325135#8325135. "
      ],
      "metadata": {
        "id": "qHQ6zlWvtGOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from io import StringIO"
      ],
      "metadata": {
        "id": "6GST-O5OtXpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learned about this from: https://stackoverflow.com/questions/28200366/python-3-x-importerror-no-module-named-cstringio."
      ],
      "metadata": {
        "id": "pYOasM0stuFC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkLvh9wnVHZ7"
      },
      "outputs": [],
      "source": [
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "\n",
        "def convert_pdf_to_txt(path):\n",
        "    rsrcmgr = PDFResourceManager()\n",
        "    retstr = StringIO()\n",
        "    codec = 'utf-8'\n",
        "    laparams = LAParams(char_margin = 20) \n",
        "    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
        "    fp = open('', 'rb') # have to put file path in here\n",
        "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "    password = \"\"\n",
        "    maxpages = 0\n",
        "    caching = True\n",
        "    pagenos=set()\n",
        "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
        "        interpreter.process_page(page)\n",
        "    fp.close()\n",
        "    device.close()\n",
        "    str = retstr.getvalue()\n",
        "    retstr.close()\n",
        "    return str"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here is where we get a string from the PDF, now need to export the string as txt\n",
        "convert_pdf_to_txt(fileReader)\n",
        "=convert_pdf_to_txt(fileReader) # decide on new object name for converted file at the start here\n",
        "print() # object to make sure it comes out as a string. this works! just takes a little bit of time"
      ],
      "metadata": {
        "id": "KrvmBs7KuL8m",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning\n",
        "In this section, I  case, lemm, and convert the cleaned files for analysis. "
      ],
      "metadata": {
        "id": "QZE-FF_aEdwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SW839cKZGoVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YAzwHkfOGn7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "G2VbS_LqFhpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "ySQRYzdsFhk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords function.\n",
        "\n",
        "def remove_Stopwords(text):\n",
        "    stop_words = set(stopwords.words('english')) \n",
        "    words = word_tokenize( text.lower() ) \n",
        "    sentence = [w for w in words if not w in stop_words]\n",
        "    return \" \".join(sentence)\n",
        "\n",
        "# Lemmatize function.    \n",
        "def lemmatize_text(text):\n",
        "    wordlist=[]\n",
        "    lemmatizer = WordNetLemmatizer() \n",
        "    sentences=sent_tokenize(text)\n",
        "    for sentence in sentences:\n",
        "        words=word_tokenize(sentence)\n",
        "        for word in words:\n",
        "            wordlist.append(lemmatizer.lemmatize(word))\n",
        "    return ' '.join(wordlist) \n",
        "\n",
        "# Cleaning text function.\n",
        "def clean_text(text): \n",
        "    delete_dict = {sp_character: '' for sp_character in string.punctuation} \n",
        "    delete_dict[' '] = ' ' \n",
        "    table = str.maketrans(delete_dict)\n",
        "    text = text.translate(table)\n",
        "    textArr= text.split()\n",
        "    text= ' '.join([w for w in textArr]) \n",
        "    \n",
        "    return text.lower()"
      ],
      "metadata": {
        "id": "HqjlBfLAFhaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Uadii1B1FhND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis\n",
        "In this section, I will analyze the cleaned files with some basic word counts to highlight significant terms from each of the reports. I will also create a few WordCloud visualizations for each report, one with all collocates true and  two or three that remove disproportionately represented terms and bigrams."
      ],
      "metadata": {
        "id": "vcaBX3NEEg05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "words = .split() # need to assign object to function, from week 4 codebook\n",
        "\n",
        "types = Counter(words)\n",
        "\n",
        "print(types)"
      ],
      "metadata": {
        "id": "PhGuOpneGB0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # from bibliometric analysis codebook\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "FPOb44jvGBxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist # from week 4 codebook\n",
        "freq = FreqDist() # need to assign an object here\n",
        "print (freq.most_common(100))\n"
      ],
      "metadata": {
        "id": "GhG_-OOdGBub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list(freq.most_common(20))) # from week 4 codebook, can use to make dataframe then list of most frequent words (in the cleaned files)"
      ],
      "metadata": {
        "id": "eQk3dglyGBkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(background_color=(246,235,189),max_words=10, width=1600, height=800,random_state=30,stopwords=[]).generate(' '.join(df[''].tolist())) # need to get reports into a dataframe of some kind\n",
        "plt.figure( figsize=(20,10))\n",
        "plt.imshow(wordcloud);"
      ],
      "metadata": {
        "id": "kWwvvORzGJ7T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}