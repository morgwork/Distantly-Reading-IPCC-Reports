{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IPCC Project CB2_Cleaning (4_9)",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMzGOcQCz8CDjvpOIdwIHBQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morgwork/Distantly-Reading-IPCC-Reports/blob/main/IPCC_Project_CB2_Cleaning_(4_9).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In this codebook, I will clean the converted files. This will involve casing, lemming, and the removal of special characters left over from PDF conversion. I will then export these files for applied analysis and visualization in a separate codebook."
      ],
      "metadata": {
        "id": "Bzdcdsg1pZwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(, \"r\", encoding='utf-8') as text:\n",
        "    novel = text.read() # use this to open .txt files"
      ],
      "metadata": {
        "id": "sLQpPzv7rNRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "eEuco5FK8uMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "WAciEpxx8xgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords function.\n",
        "\n",
        "def remove_Stopwords(text ):\n",
        "    stop_words = set(stopwords.words('english')) \n",
        "    words = word_tokenize( text.lower() ) \n",
        "    sentence = [w for w in words if not w in stop_words]\n",
        "    return \" \".join(sentence)\n",
        "\n",
        "# Lemmatize function.    \n",
        "def lemmatize_text(text):\n",
        "    wordlist=[]\n",
        "    lemmatizer = WordNetLemmatizer() \n",
        "    sentences=sent_tokenize(text)\n",
        "    for sentence in sentences:\n",
        "        words=word_tokenize(sentence)\n",
        "        for word in words:\n",
        "            wordlist.append(lemmatizer.lemmatize(word))\n",
        "    return ' '.join(wordlist) \n",
        "\n",
        "# Cleaning text function.\n",
        "def clean_text(text ): \n",
        "    delete_dict = {sp_character: '' for sp_character in string.punctuation} \n",
        "    delete_dict[' '] = ' ' \n",
        "    table = str.maketrans(delete_dict)\n",
        "    text1 = text.translate(table)\n",
        "    textArr= text1.split()\n",
        "    text2 = ' '.join([w for w in textArr]) \n",
        "    \n",
        "    return text2.lower()"
      ],
      "metadata": {
        "id": "ZfTXWEjZ83yR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}